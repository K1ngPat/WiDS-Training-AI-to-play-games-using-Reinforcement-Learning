{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(s): # inputs 210 x 160 x 3 and returns 105 x 80\n",
    "    h = 210\n",
    "    w = 160\n",
    "    \n",
    "    s = [[0.2989*s[2*i][2*j][0] + 0.5870*s[2*i][2*j][1] + 0.1140*s[2*i][2*j][2] for j in range(int(w / 2))] for i in range(int(h / 2))]\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, action_space, hidden_layers, activation_fn = nn.ReLU):\n",
    "        super(Network, self).__init__()\n",
    "        layers = [nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(8,8), stride=4), activation_fn()]\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear( 1976, hidden_layers[0]))\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "            layers.append(activation_fn())\n",
    "        \n",
    "        self.sequential = nn.Sequential(*layers)\n",
    "        self.value_output = nn.Linear(hidden_layers[-1], 1)\n",
    "        self.advantage_output = nn.Linear(hidden_layers[-1], action_space)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        state = x\n",
    "        \n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            if not(len(state) == 420):\n",
    "                tw = [[[float(k) for k in j] for j in i] for i in state]\n",
    "            else:\n",
    "                tw = state\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            state = torch.FloatTensor(tw)\n",
    "            if len(state) == 72:\n",
    "                state  = state.unsqueeze(1)\n",
    "            else:\n",
    "                state = state.unsqueeze(0)\n",
    "        \n",
    "        hidden_output = self.sequential(state)\n",
    "        value_fn = self.value_output(hidden_output)\n",
    "        advantage_fn = self.advantage_output(hidden_output)\n",
    "        \n",
    "        value_fn = value_fn.expand(advantage_fn.size())\n",
    "        action_fn = value_fn + advantage_fn - advantage_fn.mean(-1, keepdim=True).expand(advantage_fn.size())\n",
    "        return(action_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN():\n",
    "    def __init__(self, env):\n",
    "        self.env_name = env\n",
    "        self.env = gym.make(env)\n",
    "        self.state = self.env.reset()\n",
    "        self.state = [12]\n",
    "        self.online_model = Network(len(self.state), self.env.action_space.n, (128, 64))\n",
    "        self.target_model = Network(len(self.state), self.env.action_space.n, (128, 64))\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.epsilon = 0.6\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay = (self.epsilon - self.min_epsilon) / 15000\n",
    "        self.gamma = 0.999\n",
    "        self.tau = 0.03\n",
    "        self.lr = 1e-4\n",
    "        self.buffer_size = int(1e5)\n",
    "        self.min_buffer_size = 100\n",
    "        self.mini_batch_size = 72\n",
    "        \n",
    "        #Other variables\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), self.lr)\n",
    "        self.loss_fn = nn.HuberLoss(delta=1.0)\n",
    "        self.replay_buffer = collections.deque(maxlen=self.buffer_size)\n",
    "        self.episode_scores = [0.0]\n",
    "        \n",
    "        self.eval_returns = []\n",
    "    \n",
    "    def optimise(self, experiences):\n",
    "        states = [experience[0] for experience in experiences]\n",
    "        actions = [experience[1] for experience in experiences]\n",
    "        rewards = [experience[2] for experience in experiences]\n",
    "        next_states = [experience[3] for experience in experiences] \n",
    "        is_terminals = [experience[4] for experience in experiences]\n",
    "        \n",
    "        action_values = self.online_model(np.array(states)).squeeze()\n",
    "        idxs = torch.tensor(actions).long().unsqueeze(1)\n",
    "        action_values = action_values.gather(1, idxs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_action_indices = np.array([self.online_model(a).detach() for a in next_states]).squeeze()\n",
    "            next_action_indices = [ [next_action_indices[i][0][j] for j in range(4)] for i in range(72)]\n",
    "            \n",
    "            next_action_indices = np.argmax(next_action_indices, axis = 1)\n",
    "            next_action_values = np.array([self.target_model(a).detach() for a in next_states]).squeeze()\n",
    "            next_action_values = [next_action_values[p][0][next_action_indices[p]] for p in range(len(next_action_indices))]\n",
    "\n",
    "            \n",
    "        target = np.array(rewards) + self.gamma*np.array(next_action_values)*(1-np.array(is_terminals))\n",
    "        \n",
    "        target = torch.from_numpy(target).unsqueeze(1).float()\n",
    "        \n",
    "        loss = self.loss_fn(action_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step() \n",
    "        \n",
    "    def step(self, optimise  = True, update = True):\n",
    "\n",
    "        if random.random() <= self.epsilon or not(len(self.state) == 420):\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action_values = self.online_model(self.state).detach()\n",
    "                action = int(torch.argmax(action_values.squeeze()))\n",
    "        \n",
    "        ns = []\n",
    "        i = 0\n",
    "\n",
    "        while i<4:\n",
    "            mew, reward, terminated, info = self.env.step(action)\n",
    "            \n",
    "            mew = prep(mew)\n",
    "            for x in mew:\n",
    "\n",
    "                ns.append(x)\n",
    "                \n",
    "            self.episode_scores[-1] += reward\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "            \n",
    "            if terminated:\n",
    "                self.state = self.env.reset()\n",
    "                self.state = [12]\n",
    "                self.episode_scores.append(0.0)\n",
    "                break\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.epsilon = max(self.epsilon - self.decay, self.min_epsilon)\n",
    "        next_state = ns\n",
    "\n",
    "        if len(next_state) == 420 and len(self.state) == 420:\n",
    "                self.replay_buffer.append((self.state, action, reward, next_state, terminated))\n",
    "            \n",
    "        self.state = next_state\n",
    "            \n",
    "        if optimise and len(self.replay_buffer) >= self.min_buffer_size:\n",
    "            self.optimise(random.sample(self.replay_buffer, self.mini_batch_size))\n",
    "        \n",
    "        if update:\n",
    "            for target_param, online_param in zip(self.target_model.parameters(), self.online_model.parameters()):\n",
    "                target_param.data.copy_(self.tau*online_param.data + (1.0-self.tau)*target_param.data)\n",
    "    \n",
    "    # def eval_model(self, render):\n",
    "    #     with torch.no_grad():\n",
    "    #         eval_env = gym.make(self.env_name, render_mode = \"human\" if render else None)\n",
    "    #         eval_episodes = 10\n",
    "    #         for episode in range(eval_episodes):\n",
    "    #             state = prep(eval_env.reset()[0])\n",
    "    #             done = False\n",
    "    #             total_return = 0\n",
    "    #             while not done:\n",
    "    #                 action_values = self.online_model(state)\n",
    "    #                 action = int(torch.argmax(action_values.squeeze()))\n",
    "    #                 next_state, reward, terminated, truncated , info = eval_env.step(action)\n",
    "    #                 next_state = prep(next_state)\n",
    "    #                 if render:\n",
    "    #                     eval_env.render()\n",
    "    #                 done = terminated or truncated\n",
    "                    \n",
    "    #                 total_return += reward\n",
    "    #                 state = next_state\n",
    "    #             self.eval_returns.append(total_return)\n",
    "    #         return np.mean(self.eval_returns[len(self.eval_returns) - eval_episodes: ])\n",
    "    \n",
    "    def plot_results(self):\n",
    "        plt.plot(range(len(self.episode_scores)), self.episode_scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.show()\n",
    "                \n",
    "    def train(self, time_stamps, eval_time_stamps, plot_time_stamps):\n",
    "        for time_stamp in tqdm(range(1, time_stamps + 1)):\n",
    "            self.step()\n",
    "            \n",
    "            # if(time_stamp % eval_time_stamps) == 0:\n",
    "            #     if self.eval_model(False) > 480.0:\n",
    "            #         print(\"Solved CartPole with final average greater than 480 across 10 episodes\")\n",
    "            #         self.plot_results()\n",
    "            #         break\n",
    "            \n",
    "            if(time_stamp % plot_time_stamps) == 0:\n",
    "                self.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DuelingDQN(\"ALE/Breakout-v5\")\n",
    "agent.train(30000, 250, 200)\n",
    "\n",
    "torch.save(agent.online_model, \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
